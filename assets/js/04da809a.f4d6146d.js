"use strict";(self.webpackChunkdocs=self.webpackChunkdocs||[]).push([[578],{3905:(e,n,t)=>{t.d(n,{Zo:()=>p,kt:()=>m});var a=t(7294);function r(e,n,t){return n in e?Object.defineProperty(e,n,{value:t,enumerable:!0,configurable:!0,writable:!0}):e[n]=t,e}function o(e,n){var t=Object.keys(e);if(Object.getOwnPropertySymbols){var a=Object.getOwnPropertySymbols(e);n&&(a=a.filter((function(n){return Object.getOwnPropertyDescriptor(e,n).enumerable}))),t.push.apply(t,a)}return t}function l(e){for(var n=1;n<arguments.length;n++){var t=null!=arguments[n]?arguments[n]:{};n%2?o(Object(t),!0).forEach((function(n){r(e,n,t[n])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(t)):o(Object(t)).forEach((function(n){Object.defineProperty(e,n,Object.getOwnPropertyDescriptor(t,n))}))}return e}function i(e,n){if(null==e)return{};var t,a,r=function(e,n){if(null==e)return{};var t,a,r={},o=Object.keys(e);for(a=0;a<o.length;a++)t=o[a],n.indexOf(t)>=0||(r[t]=e[t]);return r}(e,n);if(Object.getOwnPropertySymbols){var o=Object.getOwnPropertySymbols(e);for(a=0;a<o.length;a++)t=o[a],n.indexOf(t)>=0||Object.prototype.propertyIsEnumerable.call(e,t)&&(r[t]=e[t])}return r}var c=a.createContext({}),s=function(e){var n=a.useContext(c),t=n;return e&&(t="function"==typeof e?e(n):l(l({},n),e)),t},p=function(e){var n=s(e.components);return a.createElement(c.Provider,{value:n},e.children)},u={inlineCode:"code",wrapper:function(e){var n=e.children;return a.createElement(a.Fragment,{},n)}},d=a.forwardRef((function(e,n){var t=e.components,r=e.mdxType,o=e.originalType,c=e.parentName,p=i(e,["components","mdxType","originalType","parentName"]),d=s(t),m=r,k=d["".concat(c,".").concat(m)]||d[m]||u[m]||o;return t?a.createElement(k,l(l({ref:n},p),{},{components:t})):a.createElement(k,l({ref:n},p))}));function m(e,n){var t=arguments,r=n&&n.mdxType;if("string"==typeof e||r){var o=t.length,l=new Array(o);l[0]=d;var i={};for(var c in n)hasOwnProperty.call(n,c)&&(i[c]=n[c]);i.originalType=e,i.mdxType="string"==typeof e?e:r,l[1]=i;for(var s=2;s<o;s++)l[s]=t[s];return a.createElement.apply(null,l)}return a.createElement.apply(null,t)}d.displayName="MDXCreateElement"},5888:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>c,contentTitle:()=>l,default:()=>u,frontMatter:()=>o,metadata:()=>i,toc:()=>s});var a=t(7462),r=(t(7294),t(3905));const o={id:"kubernetes",title:"Kubernetes",sidebar_position:1},l="Deployment With Kubernetes",i={unversionedId:"deployment/kubernetes",id:"deployment/kubernetes",title:"Kubernetes",description:"Confluent provides docker images which we can use to deploy our connector",source:"@site/docs/deployment/kubernetes.md",sourceDirName:"deployment",slug:"/deployment/kubernetes",permalink:"/kafka-connect-azure-blob-storage/docs/deployment/kubernetes",draft:!1,editUrl:"https://github.com/CoffeeBeansLabs/kafka-connect-azure-blob-storage/docs/deployment/kubernetes.md",tags:[],version:"current",sidebarPosition:1,frontMatter:{id:"kubernetes",title:"Kubernetes",sidebar_position:1},sidebar:"tutorialSidebar",next:{title:"Kafka Console Script",permalink:"/kafka-connect-azure-blob-storage/docs/deployment/kafka-console-script"}},c={},s=[{value:"Building project",id:"building-project",level:2},{value:"Building docker image",id:"building-docker-image",level:2},{value:"Copying schema files",id:"copying-schema-files",level:3},{value:"Building Image",id:"building-image",level:3},{value:"Pushing to registry",id:"pushing-to-registry",level:3},{value:"Writing kubernetes deployment file",id:"writing-kubernetes-deployment-file",level:2},{value:"Deploying",id:"deploying",level:2},{value:"Creating deployment",id:"creating-deployment",level:3},{value:"Deleting deployment",id:"deleting-deployment",level:3}],p={toc:s};function u(e){let{components:n,...t}=e;return(0,r.kt)("wrapper",(0,a.Z)({},p,t,{components:n,mdxType:"MDXLayout"}),(0,r.kt)("h1",{id:"deployment-with-kubernetes"},"Deployment With Kubernetes"),(0,r.kt)("p",null,"Confluent provides docker ",(0,r.kt)("a",{parentName:"p",href:"https://github.com/confluentinc/kafka-images"},"images")," which we can use to deploy our connector\nwithout much effort."),(0,r.kt)("p",null,"The source code includes a ",(0,r.kt)("a",{parentName:"p",href:"https://github.com/CoffeeBeansLabs/kafka-connect-azure-blob-storage/blob/main/Dockerfile"},"Dockerfile")," which extends the Confluent provided cp-kafka-connect\ndocker image and copies the connector jar and the dependency jars to the kafka-connect plugin path."),(0,r.kt)("admonition",{type:"info"},(0,r.kt)("p",{parentName:"admonition"},(0,r.kt)("inlineCode",{parentName:"p"},"kafka-connect-azure-blob-storage-${CONNECTOR_VERSION}-package")," does not include\nkafka and confluent dependencies as it is provided by the confluent image")),(0,r.kt)("br",null),(0,r.kt)("hr",null),(0,r.kt)("br",null),(0,r.kt)("p",null,"To use the provided Dockerfile and confluent provided docker images follow the steps mentioned below:"),(0,r.kt)("h2",{id:"building-project"},"Building project"),(0,r.kt)("p",null,"To create the connector and dependency jars run the following command from the project root directory:"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-bash"},"./mvnw clean install\n")),(0,r.kt)("p",null,"This generates two directories with jars in the target directory:"),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("inlineCode",{parentName:"li"},"kafka-connect-azure-blob-storage-${CONNECTOR_VERSION}-development")),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("inlineCode",{parentName:"li"},"kafka-connect-azure-blob-storage-${CONNECTOR_VERSION}-package"))),(0,r.kt)("h2",{id:"building-docker-image"},"Building docker image"),(0,r.kt)("h3",{id:"copying-schema-files"},"Copying schema files"),(0,r.kt)("p",null,"Copying schemas are only relevant if using ",(0,r.kt)("inlineCode",{parentName:"p"},"topic-name.schema.url")," configuration.\nConfiguration is relevant only in case when incoming data is of type Json-string\nand format is either Parquet or Avro."),(0,r.kt)("p",null,"The schemas (file) can be passed to the configuration as below example:"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-yaml"},"topic-name.schema.url: file:///usr/share/java/schema-file\n")),(0,r.kt)("p",null,"Copying of schema is not needed if schema will be fetched from a remote service"),(0,r.kt)("p",null,"The schemas (remote service) can be passed to the configuration as below example:"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-yaml"},"topic-name.schema.url: https://link/to/schema-file\n")),(0,r.kt)("p",null,"Make changes to the Dockerfile according to your use-case.\nIf changing the destination directory, make sure it has required access permissions"),(0,r.kt)("br",null),(0,r.kt)("h3",{id:"building-image"},"Building Image"),(0,r.kt)("p",null,"To build the docker image from the provided Dockerfile, run this command from the project root directory :"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-zsh"},"docker build . -t {Your docker image name}:{Version} --build-arg CONNECTOR_VERSION={Connector version}\n")),(0,r.kt)("p",null,"Replace"),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("inlineCode",{parentName:"li"},"{Your docker image name}")," with your docker image name"),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("inlineCode",{parentName:"li"},"{Version}")," with version of the docker image"),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("inlineCode",{parentName:"li"},"{Connector version}")," with your connector jar version")),(0,r.kt)("br",null),(0,r.kt)("h3",{id:"pushing-to-registry"},"Pushing to registry"),(0,r.kt)("p",null,"To push this docker image to the registry use this command :"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-zsh"},"docker push {Your docker image tag}\n")),(0,r.kt)("h2",{id:"writing-kubernetes-deployment-file"},"Writing kubernetes deployment file"),(0,r.kt)("p",null,"Below is a ",(0,r.kt)("b",null,"sample")," kubernetes deployment configuration. Actual configuration may differ based\non your requirements."),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-yaml"},'apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: kafka-connect-azure-blob-storage\n  namespace: development\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: kafka-connect-azure-blob-storage\n  template:\n    metadata:\n      labels:\n        app: kafka-connect-azure-blob-storage\n    spec:\n      containers:\n      - env:\n        - name: CONNECT_REST_ADVERTISED_HOST_NAME\n          valueFrom:\n            fieldRef:\n             fieldPath: status.podIP\n        - name: CONNECT_BOOTSTRAP_SERVERS\n          value: "kafka.broker.address"\n        - name: CONNECT_GROUP_ID\n          value: "kcabs-cluster"\n        - name: CONNECT_CONFIG_STORAGE_TOPIC\n          value: "_kcabs-connect-configs"\n        - name: CONNECT_OFFSET_STORAGE_TOPIC\n          value: "_kcabs-connect-offset"\n        - name: CONNECT_STATUS_STORAGE_TOPIC\n          value: "_kcabs-connect-status"\n        - name: CONNECT_KEY_CONVERTER\n          value: "org.apache.kafka.connect.storage.StringConverter"\n        - name: CONNECT_VALUE_CONVERTER\n          value: "org.apache.kafka.connect.storage.StringConverter"\n        image: docker-image\n        imagePullPolicy: IfNotPresent\n        name: kafka-connect-azure-blob-storage\n        ports:\n          - containerPort: 8083\n        resources:\n          limits:\n            memory: 4Gi\n          requests:\n            memory: 2Gi\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: kafka-connect-azure-blob-storage\n  namespace: development\nspec:\n  selector:\n    app: kafka-connect-azure-blob-storage\n  ports:\n    - name: http\n      port: 8083\n  type: ClusterIP\n')),(0,r.kt)("p",null,"Replace:"),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("inlineCode",{parentName:"li"},"CONNECT_BOOTSTRAP_SERVERS")," value to kafka broker address"),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("inlineCode",{parentName:"li"},"image")," value to your docker image address")),(0,r.kt)("br",null),(0,r.kt)("p",null,"The below configuration sets the pod IP address as environment variable with key ",(0,r.kt)("inlineCode",{parentName:"p"},"CONNECT_REST_ADVERTISED_HOST_NAME")),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-yaml"},"env:\n  - name: CONNECT_REST_ADVERTISED_HOST_NAME\n    valueFrom:\n      fieldRef:\n        fieldPath: status.podIP\n")),(0,r.kt)("p",null,"All pods have unique ",(0,r.kt)("inlineCode",{parentName:"p"},"CONNECT_REST_ADVERTISED_HOST_NAME")," which they broadcast to the kafka-connect cluster\nwhich they are part of. It is used for communication between worker nodes."),(0,r.kt)("h2",{id:"deploying"},"Deploying"),(0,r.kt)("h3",{id:"creating-deployment"},"Creating deployment"),(0,r.kt)("p",null,"To perform the deploy run the below command :"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-bash"},"kubectl apply -f configuration.yaml\n")),(0,r.kt)("h3",{id:"deleting-deployment"},"Deleting deployment"),(0,r.kt)("p",null,"To delete the deployment and remove pods run the below command:"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-bash"},"kubectl delete -f configuration.yaml\n")))}u.isMDXComponent=!0}}]);